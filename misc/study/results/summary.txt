- Large Language Models (LLMs) are trained on vast amounts of text data, such as GPT-3 on 500 billion tokens.
- They learn by predicting the next word in a sequence, using self-supervised learning.
- Transformers are the core architecture of modern LLMs, utilizing attention mechanisms instead of recurrence or convolutions.
- Attention allows models to selectively focus on relevant parts of input data, enabling contextual understanding.
- Contextual embeddings differ from static embeddings by changing based on surrounding words, improving meaning representation.
- Self-attention computes importance scores between words using dot products and normalizes them with softmax.
- Transformers divide input vectors into three roles: query, key, and value, each projected through different weight matrices.
- Large models are pretrained on massive datasets like the Pile, containing diverse sources for broad knowledge.
- Pretraining involves predicting the next word, optimizing via cross-entropy loss.
- Approaches vary by architecture: decoders (causal models), encoders (bidirectional models), and encoder-decoder models.
- Decoder models generate text autoregressively, suitable for tasks like language generation.
- Encoder models capture full context bidirectionally, good for classification tasks.
- Encoder-decoder models excel in mapping between sequences, e.g., translation.
- Fine-tuning and prompting adapt pretrained models for specific downstream tasks.
- Ethical issues include hallucination, misinformation, privacy concerns, and potential harm.
- Retrieval-Augmented Generation (RAG) improves accuracy by retrieving relevant information during generation.
- Future courses will focus on advanced NLP techniques, social applications, conversational AI, and ethical considerations.