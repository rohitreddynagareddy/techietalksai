<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Learn-train-LLM-from basics</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<h1 id="toc_0">Learn LLM from Basics</h1>

<hr>

<h2 id="toc_1">1. Project Layout &amp; Getting Started</h2>

<hr>

<p>Folder contents:</p>

<div><pre><code class="language-none">â€¢Â Dockerfile &amp; dockerâ€‘compose.yml  â†’ isolate your environment
â€¢Â run.sh                           â†’ convenience wrapper (sh run.sh step2_tokenizer.py)
â€¢Â step1_env_check.py â€¦ step12_miniâ€‘decoder.py â†’ twelve incremental Python scripts
â€¢Â mini_gpt.pth                     â†’ trained weights from step7
â€¢Â notes.txt                        â†’ your personal plan

Prerequisites:
â€¢Â Gitâ€‘clone this folder
â€¢Â Have Docker (optional) or a local PyTorch install
â€¢Â (Optionally) run docker compose up step1 --build to verify PyTorch &amp; MPS/CPU</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 1: Environment Check (step1_env_check.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ make sure PyTorch is installed and pick the right device (CPU / GPU / Apple MPS)

Key concepts:
â€¢Â torch.version
â€¢Â torch.device(...)
â€¢Â tensors live on â€œdeviceâ€ (CPU vs GPU vs MPS)

Running it prints your PyTorch version, host architecture, and creates a toy tensor to confirm everything works.</code></pre></div>

<hr>

<div><pre><code class="language-none">1. StepÂ 2: Tiny Corpus &amp; Tokenizer (step2_tokenizer.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ turn text into numbers (â€œtokenizationâ€)

Code highlights:
â€¢Â corpus = â€œI am Sree from Schogini.comâ€
â€¢Â vocab = sorted(set(corpus))  â” all unique characters
â€¢Â char_to_idx, idx_to_char dicts
â€¢Â encoded = torch.tensor([char_to_idx[ch] for ch in corpus])
â€¢Â decoded back again with idx_to_char

Key concepts for beginners:
â€¢Â Every NLP model works on numbers, not raw strings.
â€¢Â Token = smallest unit (here, a character).
â€¢Â Mapping str â†” int lets us embed them in a neural net.</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 3: Tiny Transformer (step3_tiny_transformer.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ define the simplest â€œtransformerâ€styleâ€ network: embed â†’ linear â†’ logits

Network:
â€¢Â nn.Embedding(vocab_size, embedding_dim)
â€¢Â nn.Linear(embedding_dim, vocab_size)

Forward pass:

    1. take a batch of token IDs
    2. embed into 16â€‘dim vectors
    3. project back to â€œvocab_sizeâ€ scores (logits)

Logits are raw scores youâ€™ll convert with softmax later.</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 4: Forward Pass with Debugging (step4_forward_pass.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ walk through shapes &amp; values

Adds:
â€¢Â builds tiny vocab from â€œhello worldâ€
â€¢Â prints out shapes at each stage, e.g.
   â€“ Input IDs: [5]
   â€“ After Embedding: [5Ã—16]
   â€“ After Linear: [5Ã—vocab_size]

Why it matters:
â€¢Â Verifying dimensions is the single most important debugging trick in deep learning.</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 5: Tiny Training Loop (step5_train_simple.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ teach the model to predict next character in â€œhelloâ€

Pipeline:

    1. encode â€œhelloâ€ â†’ [h, e, l, l, o]
    2. x = first 4 tokens, y = last 4 tokens shifted by one
    3. forward(x) â” logits of shape [4Ã—vocab_size]
    4. CrossEntropyLoss(logits, y)
    5. backprop + optimizer.step()
    6. print loss every 10 epochs

Key concepts:
â€¢Â CrossEntropyLoss combines softmax + negativeâ€‘logâ€‘likelihood
â€¢Â optimizer.zero_grad(), loss.backward(), optimizer.step()
â€¢Â predict nextâ€‘token is the core of autoregressive language models</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 6: Simple Inference &amp; Greedy Decoding (step6_simple_inference.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ after training, generate a sequence characterâ€‘byâ€‘character

Greedy decoding:

    1. start with a single token (e.g. â€œhâ€)
    2. model(input_ids) â” logits
    3. softmax + argmax â” index of most probable next char
    4. append &amp; repeat

Youâ€™ll see the model output â€œhellllllllllâ€¦â€, a sign it learned the pattern.</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 7: Build a Miniâ€‘GPT (step7_mini_gpt.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ a minimal GPTâ€‘style block with:
â€¢Â token + position embeddings
â€¢Â causal selfâ€‘attention (nn.MultiheadAttention)
â€¢Â feedâ€‘forward (MLP)
â€¢Â layer norms &amp; residual connections

Details:
â€¢Â block_size = context window length
â€¢Â position_embedding = nn.Embedding(2Ã—block_size, dim)  (avoid overflow)
â€¢Â attn_mask = upperâ€triangular inf mask for causality
â€¢Â train on random 8â€‘token chunks of â€œhello worldâ€ for 500 epochs
â€¢Â save model.state_dict() â†’ mini_gpt.pth

Key concepts:
â€¢Â Positional encodings let the model know â€œwhereâ€ tokens sit.
â€¢Â Multiâ€‘head selfâ€‘attention builds contextualized representations.</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 8: Bonusâ€”Inspecting Oneâ€‘Head Attention (step8_bonus_multihead_mini_gpt.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ step through oneâ€‘head attention + residuals by printing intermediate shapes

Adds:
â€¢Â print after embedding+positional, after attention, after feedâ€‘forward
â€¢Â single head (num_heads=1) for simplicity
â€¢Â slide a 1â€‘batch training loop

This reinforces your mental model of the Transformer decoder block.</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 9: Load &amp; Infer from Your Saved GPT (step9_load_and_infer.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ reâ€‘instantiate the same MiniGPT class, load mini_gpt.pth, and sample

Key points:
â€¢Â Your class definition must match exactly
â€¢Â model.load_state_dict(torch.load(&quot;mini_gpt.pth&quot;))
â€¢Â Use multinomial sampling for randomness
â€¢Â Maintain a sliding context window to avoid infinite growth</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 10: Topâ€‘k Sampling &amp; Long Context (step10_upgraded_top-k-sampling-long-context.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ replace greedy decoding with topâ€‘k sampling and enforce a fixed context window

Topâ€‘k sampling:

    1. take logits for last token
    2. pick the topÂ k highest scores
    3. renormalize with softmax
    4. sample from that smaller distribution

Long context:
â€¢Â always keep only the last block_size tokens as input

This mimics GPTâ€‘style generation more closely.</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 11: Temperature Control (step11_temperature.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ add temperature to control randomness

Temperature scaling:
â€¢Â logits â† logits / T
   â€“ TÂ &gt;Â 1 â†’ flattens (more random)
   â€“ TÂ &lt;Â 1 â†’ sharpens (more deterministic)

Combined with topâ€‘k, this gives you fine control over creativity vs coherence.</code></pre></div>

<hr>

<div><pre><code class="language-none">    1. StepÂ 12: Building a Mini Decoder Block (step12_miniâ€‘decoder.py)</code></pre></div>

<hr>

<div><pre><code class="language-none">Goal  â†’ pull everything together into a reusable MiniDecoderBlock:

Layers:

    1. token + positional embeddings
    2. causal multiâ€‘head attention (mask via a helper)
    3. add &amp; layerâ€‘norm
    4. feedâ€‘forward MLP
    5. add &amp; layerâ€‘norm

You run it on dummy input (batch_size=2, seq_len=5) to confirm you get a (2Ã—5Ã—dim) output.</code></pre></div>

<hr>

<div><pre><code class="language-none">## Wrapping Up

â€¢Â You now have a characterâ€‘level mini GPT from scratch.
â€¢Â Youâ€™ve seen how tokenization, embeddings, attention, training loops, and sampling strategies all fit together.
â€¢Â Try extending it: bigger vocab, different temperature schedules, beam search, or full wordâ€‘level tokenization.

Happy hacking! ğŸš€</code></pre></div>




</body>

</html>
